# ğŸ” PRISM - Provider Resource Issue Scanning & Monitoring

**PRISM** is an intelligent multi-agent system built with Strands SDK that automatically analyzes and tests AWSCC (AWS Cloud Control) provider issues from the Terraform provider repository. It helps maintainers quickly understand, reproduce, and validate configuration problems using the latest provider versions.

## ğŸ¯ What PRISM Does

- ğŸ” **Issue Discovery**: Automatically fetches issues labeled `needs-triage` from the terraform-provider-awscc repository
- ğŸš€ **Latest Provider Testing**: Always uses the latest AWSCC provider version for accurate testing
- âš™ï¸ **Configuration Testing**: Extracts Terraform configurations from issue descriptions and tests them in isolated environments
- ğŸ¤– **Automated Analysis**: Runs complete Terraform lifecycle (`init`, `validate`, `plan`, `apply`, `destroy`) to reproduce reported problems
- ğŸ§¹ **Smart Cleanup**: Automatically removes large Terraform files (.terraform/, state files) to save disk space while preserving .tf files and directories for review
- ğŸ—ï¸ **Modular Architecture**: Clean separation of concerns with specialized agent modules in dedicated directory structure
- ğŸ“‹ **Documentation**: Generates comprehensive markdown reports with test results, error analysis, and reproduction steps

## âœ¨ Key Features

- ğŸ—ï¸ **Multi-Agent Architecture**: Uses specialized MCP-enabled agents for different aspects of issue analysis
- ğŸ“¦ **Latest Provider Versions**: Automatically queries and uses the latest AWSCC provider version
- ğŸ”— **GitHub Integration**: Uses GitHub MCP server to fetch and analyze issues
- ğŸ”§ **Terraform Automation**: Full Terraform lifecycle testing with proper error handling and cleanup
- ğŸ“š **AWS Documentation**: Integrates AWS documentation MCP server for expert analysis
- âš¡ **Automated Execution**: Runs without manual confirmation prompts for streamlined operation
- ğŸ¯ **Intelligent Filtering**: Excludes resource-suppression issues and focuses on recent problems
- ğŸ—‚ï¸ **Clean Environment**: Automatically cleans up large files and AWS resources after testing

## ğŸš€ Quick Start

1. **Install Dependencies**:
   ```bash
   uv sync
   ```

2. **Configure Settings** (optional):
   Edit `agent_config.json` to customize:
   - ğŸ“… Maximum age of issues to analyze
   - ğŸ”¢ Number of issues to process
   - ğŸ§  Model configuration

3. **Run Analysis**:
   ```bash
   ./run_triage.sh
   ```
   
   The system will automatically:
   - ğŸ“¥ Fetch qualifying issues using GitHub MCP
   - ğŸ”„ Get latest AWSCC provider version using Terraform MCP
   - ğŸ§ª Extract and test configurations
   - ğŸ§¹ Clean up large files and AWS resources while preserving .tf files
   - ğŸ“Š Generate comprehensive analysis reports

## âš™ï¸ Configuration

The agent behavior is controlled by `agent_config.json`:

```json
{
  "agent": {
    "name": "PRISM",
    "description": "Provider Resource Issue Scanning & Monitoring"
  },
  "github": {
    "repo": "hashicorp/terraform-provider-awscc",
    "labels": {
      "include": ["needs-triage"],
      "exclude": ["resource-suppression"]
    },
    "max_age_days": 10,
    "max_issues": 2
  },
  "model": {
    "model_id": "us.anthropic.claude-sonnet-4-20250514-v1:0",
    "temperature": 0.3,
    "max_tokens": 4096
  }
}
```

## ğŸ“¤ Output

PRISM generates:
- ğŸ“ **Individual test directories**: `issue_<id>/` for each analyzed issue with preserved .tf files
- ğŸ“‹ **Triage reports**: `triage_issue_<id>.md` with detailed analysis
- ğŸ§¹ **Clean workspace**: Large Terraform files (.terraform/, state files) automatically removed after testing while preserving .tf files for review

## ğŸ“‹ Requirements

- ğŸ Python 3.12+ with [uv](https://docs.astral.sh/uv/) package manager
- ğŸ—ï¸ Terraform CLI
- â˜ï¸ AWS CLI configured
- ğŸ”— GitHub access (for MCP server)
- ğŸ§¬ Strands SDK with strands-agents-tools
- ğŸ³ Docker/Finch (for Terraform MCP server)

## ğŸ—ï¸ Architecture

PRISM uses a multi-agent MCP-enabled approach with a clean modular structure:

### Project Structure
```
prism/
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ agents/                    # Agent modules directory
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ orchestrator.py       # Main coordination agent
â”‚   â”œâ”€â”€ github_agent.py       # GitHub specialist agent
â”‚   â”œâ”€â”€ terraform_agent.py    # Terraform specialist agent
â”‚   â””â”€â”€ analysis_agent.py     # Analysis specialist agent
â”œâ”€â”€ pyproject.toml           # Project configuration and dependencies
â”œâ”€â”€ uv.lock                  # Dependency lock file
â”œâ”€â”€ .python-version          # Python version specification
â”œâ”€â”€ run_triage.sh            # Execution script
â””â”€â”€ .gitignore               # Git ignore patterns
```

### Agent Responsibilities
- ğŸ¯ **Orchestrator Agent**: Coordinates the overall workflow and delegates tasks with proper resource cleanup
- ğŸ”— **GitHub Agent**: Uses GitHub MCP server to fetch and analyze issues
- ğŸ”§ **Terraform Agent**: Uses Terraform MCP server and shell tools for testing with latest provider versions, preserves .tf files while cleaning up large state files
- ğŸ“Š **Analysis Agent**: Uses AWS Documentation MCP server for expert analysis and report generation

Built with the Strands SDK for robust agent orchestration, MCP servers for external integrations, and AWS Bedrock for intelligent analysis.

## ğŸ“ TODO

- ğŸ”„ **GitHub Issue Updates**: Update GitHub issues with analysis results once complete. Functionality works but GitHub's fine-grained tokens have limitations compared to classic tokens - despite trying `issues: write` and repo-specific permissions, fine-grained tokens consistently return 403 errors
- ğŸ“¦ **S3 Storage**: Copy configuration files and analysis results to S3 bucket for deeper analysis (conditional step, nice-to-have since agents already create `issue_id` directories)
- â˜ï¸ **AWS AgentCore Migration**: Convert implementation to AWS AgentCore service
- â° **Scheduled Execution**: Set up weekly automated runs (implementation already filters by `needs-triage` and date limits)
- ğŸ“Š **Issue Tracking**: Track already-processed issues using either:
  - Storage-based tracking: Use existing issue IDs to skip duplicates
  - Label-based tracking: Update `needs-triage` label once analysis is posted (preferred if issue metadata updates work)
